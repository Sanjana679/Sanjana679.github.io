<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Exploring Reward Sharing Strategies for Effective Cooperative Multi-Agent Task Completion | Sanjana Nandi </title> <meta name="author" content="Sanjana Nandi"> <meta name="description" content="Final project for CS 5756 Introduction to Robot Learning at Cornell University (M.Eng. CS program)"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://sanjana679.github.io/projects/cs5756-final-proj/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Sanjana</span> Nandi </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/books/">bookshelf </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Exploring Reward Sharing Strategies for Effective Cooperative Multi-Agent Task Completion</h1> <p class="post-description">Final project for CS 5756 Introduction to Robot Learning at Cornell University (M.Eng. CS program)</p> </header> <article> <p>People: Juji Lau, Sanjana Nandi <br> GitHub Repository: https://github.com/juji-lau/on-policy/tree/main</p> <h2 id="1-introduction">1. Introduction</h2> <p>In many real-world situations where the task is too big for a single actor to accomplish alone, effective cooperation among multiple agents is crucial for success. A few examples of such situations include: search and rescue missions,hazardous environment cleanup, and area surveillance.</p> <p>Currently, policy gradient methods such as Proximal Policy Optimization (PPO) have proven successful for single-agent settings. PPO is a widely used policy gradient algorithm that focuses on updating the policy of individual agents based on local observations, and individual rewards. In PPO agents act independently, which works well for single-agent tasks (such as learning to walk, moving stationary objects, navigating a messy environment, etc.). However, in tasks requiring coordination among multiple agents, the lack of global context, and therefore inter-agent communication, may lead to suboptimal behaviors.</p> <p>This is where Multi-Agent Proximal Policy Optimization (MAPPO) steps in. The MAPPO algorithm modifies the PPO algorithm to better suit multi-agent environments. In MAPPO, a single critic aggregates the observations and actions from all acting agents to compute value estimates while each agent maintains its own policy network. This shared critic allows indirect ”communication” between agents, providing them a common understanding of the environment and of each others’ actions. As a result, MAPPO agents can make decisions that better account for team-level objectives, leading to more coherent group strategies, and hence an overall better task outcome.</p> <h2 id="2-problem">2. Problem</h2> <p>While the structure of MAPPO inherently encourages multi-agent cooperation, its true success in a cooperative setting depends heavily on how rewards are structured. This leads us to our research question:</p> <p><em>How do we best structure the reward function to maximize the collective performance of agents in a cooperative environment under the MAPPO framework?</em></p> <p>There are numerous possible reward functions. For this project, we grouped the possibilities into three main categories. (Let ri denote the reward given to an individual agent.):</p> <ol> <li>Purely Individual Rewards: $r_i$ = $R_i$</li> <li>Partially Shared Rewards: $r_i$ = $R_{ps}$ = some combination of $R_i$ and $R_s$</li> <li>Entirely Shared Rewards: $r_i$ = $R_s$</li> </ol> <p>We aim to determine which category of reward function (or combination of categories) leads to the most effective task completion in multi-agent, cooperation-dependent settings. We hypothesize that in such a setting, agents trained with Partially Shared Rewards ($R_{ps}$), will achieve better task coverage and cooperation than agents trained with Purely Individual Rewards ($R_i$), which in turn will outperform agents trained with Entirely Shared Rewards ($R_s$).</p> <h2 id="3-approach">3. Approach</h2> <p>To test our hypothesis, we trained agents with the Multi-Agent Proximal Policy Optimization (MAPPO) algorithm using reward function(s) from each of the three categories.</p> <h3 id="31-experimental-setup-environment">3.1 Experimental Setup: Environment</h3> <p>For our multi-agent cooperative environment, we used the <a href="https://pettingzoo.farama.org/environments/mpe/simple_spread/" rel="external nofollow noopener" target="_blank">Simple Spread</a> environment from <a href="https://pettingzoo.farama.org/}{PettingZoo" rel="external nofollow noopener" target="_blank">PettingZoo</a>’s library <a href="https://pettingzoo.farama.org/environments/mpe/" rel="external nofollow noopener" target="_blank">Multi-Agent Particle Environments (MPE)</a> <a class="citation" href="#yu2022">(Yu et al., 2022)</a> <a class="citation" href="#mordatch2017emergence">(Mordatch &amp; Abbeel, 2017)</a>.Simple Spread is a cooperative game where <em>N</em> agents must attempt to cover <em>N</em> stationary landmarks in <em>t</em> time-steps without colliding. We chose Simple Spread because it’s a <em>fully</em> cooperative task (i.e., there are no competitive elements), which encourages all agents to work towards a common goal. This setup allows us to compare the effectiveness of different reward structures without confounding factors.</p> <h3 id="32-experimental-setup-training">3.2 Experimental Setup: Training</h3> <p>To maximize the models’ performance under their given reward function, while ensuring a fair evaluation, all models were trained with the following parameters:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    --num_env_steps 100000
    --episode_length 25
    --ppo_epoch 30 # previously 15
    --clip_param 0.1 # previously 0.2
    --entropy_coef 0.005 # previously 0.01
    --lr 5e-4 # unchanged
    --critic_lr 5e-4 # unchanged
    --use_valuenorm # added to source code
    --use_feature_normalization # added to source code
    --hidden_size 128 # added to source code
    --layer_N 2 # added to source code
</code></pre></div></div> <p>If not listed above, we used the defaults provided by the MAPPO implementation <a class="citation" href="#yu2022">(Yu et al., 2022)</a> <a class="citation" href="#mordatch2017emergence">(Mordatch &amp; Abbeel, 2017)</a>. These parameters gave the best average reward and entropy across all three reward schemes. Additionally, the number of landmarks and agents is set at (N = 3) for all models.</p> <h3 id="33-experimental-setup-reward-structures">3.3 Experimental Setup: Reward Structures</h3> <p>To test our hypothesis about how reward structure influences cooperative task performance under MAPPO, we modified the official implementation of MAPPO <a class="citation" href="#lowe2017multi">(Lowe et al., 2017)</a> to train three variants, each using one of our defined reward schemes. The reward function for each structure is given below:</p> <ol> <li> <em>Purely Individual Rewards</em>: The rewards for each agent are based purely on individual performance. (This encourages selfish behavior).</li> </ol> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    def individual():
        ideal_dist = 0.25
        tolerance = 0.1
        bonus = 1.0
        penalty_close = -1.0
        penalty_far = -0.5
        collision_penalty = -1.0

        # Find nearest landmark
        distances = [np.linalg.norm(agent.state.p_pos -
                    l.state.p_pos) for l in world.landmarks]
        min_dist = min(distances)

        # Reward shaping
        if abs(min_dist - ideal_dist) &lt; tolerance:
            proximity_reward = bonus
        elif min_dist &lt; ideal_dist:
            proximity_reward = penalty_close
        else:
            proximity_reward = penalty_far

        # Penalize collisions
        collision_loss = 0
        if agent.collide:
            for other in world.agents:
                if other is not agent 
                    and self.is_collision(agent, other):
                    collision_loss += collision_penalty

        return proximity_reward + collision_loss
</code></pre></div></div> <ol> <li> <em>Partially Shared Rewards</em>: The rewards for each agent are dependent on both individual performance and collective progress. (This encourages a mix of selfish behavior and global coordination).</li> </ol> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def partially_shared():
    # Individual component
    individual_rewards = []
    for agent in world.agents:
        dists = [np.linalg.norm(agent.state.p_pos -
                l.state.p_pos) for l in world.landmarks]
        min_dist = min(dists)
        individual_rewards.append(-min_dist)

    # Shared component (mean distance to landmarks)
    total_dist = 0
    for l in world.landmarks:
        for a in world.agents:
            total_dist += np.linalg.norm(a.state.p_pos -
            l.state.p_pos)

    if not hasattr(world, "prev_total_dist") or
    world.prev_total_dist is None:
        world.prev_total_dist = total_dist

    shared_progress = world.prev_total_dist - total_dist
    world.prev_total_dist = total_dist

    # Coverage bonus: reward if each landmark is closest to a
    different agent
    closest_agents = [np.argmin([np.linalg.norm(a.state.p_pos -
        l.state.p_pos) for a in world.agents]) for l in
        world.landmarks]
    coverage_bonus = len(set(closest_agents))  # Higher if
    agents spread out

    return 0.6 * individual_rewards[world.agents.index(agent)]
    + 0.4 * (shared_progress + coverage_bonus * 0.1)`
</code></pre></div></div> <ol> <li> <em>Entirely Shared Rewards</em>: All agents receive the same reward, which is a function of collective progress. (This encourages global coordination).</li> </ol> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    def shared():
        total_dist = 0
        for l in world.landmarks:
            for a in world.agents:
                total_dist += np.linalg.norm(a.state.p_pos - l.state.p_pos)

        if not hasattr(world, "prev_total_dist") or world.prev_total_dist is None:
            world.prev_total_dist = total_dist

        progress = world.prev_total_dist - total_dist
        world.prev_total_dist = total_dist

        # Penalize collisions
        collision_penalty = 0
        for i, a1 in enumerate(world.agents):
            for j, a2 in enumerate(world.agents):
                if i != j and self.is_collision(a1, a2):
                    collision_penalty -= 1

        return progress + collision_penalty
</code></pre></div></div> <h3 id="34-experimental-setup-evaluation">3.4 Experimental Setup: Evaluation</h3> <p>To provide a consistent baseline comparison and to ensure a fair evaluation, the performance of all models were evaluated using Simple Spread’s original reward function:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def reward(self, agent, world):
    # Agents are rewarded based on minimum agent distance to each landmark:
    rew = 0
    for l in world.landmarks:
        dists = [np.sqrt(np.sum(np.square(a.state.p_pos - l.state.p_pos)))
                 for a in world.agents]
        rew -= min(dists)
        
    # Agents are penalized for collisions:
    if agent.collide:
        for a in world.agents:
            if self.is_collision(a, agent):
                rew -= 1
    return rew

</code></pre></div></div> <p>In this function, agents are globally rewarded by summing the distance between each landmark and its closest agent (encouraging coverage) and locally penalized for each collision with another agent (encouraging inter-agent cooperation).</p> <p><em>Technically agents are penalized based on their distances from the landmarks, but the statement above is conceptually correct.</em></p> <h2 id="4-results">4. Results</h2> <p>The qualitative and quantitative results across all three rewards schemes reveal that task completion and group coordination improves as the reward structure becomes more shared.</p> <h3 id="41-qualitative-results">4.1 Qualitative Results</h3> <p>The final spatial coverage achieved by 3 agents after 25 time steps in the Simple Spread environment are depicted in Figures 1, 2, and 3.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/cs5756-final-project/individual_final-480.webp 480w,/assets/img/projects/cs5756-final-project/individual_final-800.webp 800w,/assets/img/projects/cs5756-final-project/individual_final-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/projects/cs5756-final-project/individual_final.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Individual Shared Reward Result" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 1: **Purely Individual Rewards** This figure depicts the final landmark coverage of _N_ = 3 agents trained under the _purely individual reward_ function </div> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/cs5756-final-project/shared_final-480.webp 480w,/assets/img/projects/cs5756-final-project/shared_final-800.webp 800w,/assets/img/projects/cs5756-final-project/shared_final-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/projects/cs5756-final-project/shared_final.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Entirely Shared Reward Result" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 3: **Entirely Shared Rewards** This figure depicts the final landmark coverage of _N_=3 agents trained under the _entirely shared reward_ function. </div> <p>Although none of the reward schemes resulted in all of the landmarks being covered, the partially shared reward scheme came the closest, followed by entirely shared reward scheme, and then the individual reward scheme.</p> <p>With the individual reward scheme, the agents optimized their own rewards and often converged on the same landmarks, resulting in poor coverage and coordination. The partially shared reward scheme lead to an attempt at unsuccessful coordination as a result of conflicting reward signals from the individual and shared components. The shared reward scheme produced the most effective group behavior: the agents tried to distribute themselves across landmarks with minimal collisions. This seems to suggest that shared rewards can encourage cooperative behavior and more efficient task completion in multi-agent situations.</p> <h3 id="42-quantitative-results-and-analysis">4.2 Quantitative Results and Analysis</h3> <p>The average training rewards, average evaluation rewards, and entropies, for the individual, partially, and fully shared rewards are plotted in Figures 4, 5, and 6 respectively.</p> <p>The maximum entropy for choosing among <em>x</em> discrete actions is given by $ H_{max} = log(x) $</p> <p>In Simple Spread, the agents had <em>5</em> actions to choose from:</p> <ul> <li>No action</li> <li>Go up</li> <li>Go down</li> <li>Go left</li> <li>Go right</li> </ul> <p>Meaning the maximum entropy for out agents is given by $H_{max} = log(5) = 1.609$. Hence, we can use the following benchmarks to interpret the model entropy:</p> <ul> <li>$h_{model} = 1.6$: the policy is still random and exploratory</li> <li>$h_{model} = 1.0$: the policy is starting to prefer certain actions</li> <li>$h_{model} = 0.5$: the policy is very confident, often choosing the same actions</li> </ul> <p>As shown in the graphs, all reward types experience a downwawrd trajectory of entropy as the model trains. It appears that the <em>less</em> individualistic the reward, the <em>faster</em> the entropy converges to zero. In other words, individualism <em>encourages</em> model exploration.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/cs5756-final-project/individual-480.webp 480w,/assets/img/projects/cs5756-final-project/individual-800.webp 800w,/assets/img/projects/cs5756-final-project/individual-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/projects/cs5756-final-project/individual.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Individual Shared Reward Graph" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 4: **Purely Individual Rewards** This figure depicts the average training and evaluation rewards, along with the entropy of _N_ = 3 agents trained under the _purely individual reward_ function. </div> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/cs5756-final-project/partially_shared-480.webp 480w,/assets/img/projects/cs5756-final-project/partially_shared-800.webp 800w,/assets/img/projects/cs5756-final-project/partially_shared-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/projects/cs5756-final-project/partially_shared.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Entirely Shared Reward Graph" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 5: **Partially Shared Rewards** This figure depicts the average training and evaluation rewards, along with the entropy of _N_ = 3 agents trained under the _partially shared reward_ function. </div> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/cs5756-final-project/shared-480.webp 480w,/assets/img/projects/cs5756-final-project/shared-800.webp 800w,/assets/img/projects/cs5756-final-project/shared-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/projects/cs5756-final-project/shared.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Entirely Shared Reward Graph" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 6: **Entirely Shared Rewards** This figure depicts the average training and evaluation rewards, along with the entropy of _N_ = 3 agents trained under the _entirely shared reward_ function. </div> <p>In the Simple Spread environment, higher reward values indicate more successful task performance. Based on our graphs, the <em>Entirely Shared Rewards</em> performed the best, followed by the <em>Partially Shared Rewards</em>, which performed about the same as the <em>Purely Individual Rewards</em>. This was surprising as we predicted that the <em>Partially Shared Rewards</em> would outperform the <em>Purely Individual Rewards</em>, which would outperform the <em>Entirely Shared Rewards</em>.</p> <p>When we compare average returns and policy entropy convergence across all three reward schemes, a pattern emerges: the greater the emphasis on shared rewards during training, the better the task completion in a multi-agent cooperative setting. This is evidenced by the fact that the model trained with <em>Entirely Shared Rewards</em> outperformed the model trained with <em>Partially Shared Rewards</em>, followed <em>extremely</em> closely by the model trained with <em>Purely Individual Rewards</em>.</p> <h3 id="43-future-steps">4.3 Future Steps</h3> <p>Given more time, we would have liked to further train our model for all three reward structures until their average reward is closer to zero, and their entropy is less than 0.5.</p> <h2 id="5-individual-contribution">5. Individual Contribution</h2> <h3 id="51-juji-lau">5.1 Juji Lau</h3> <p>Juji took primary responsibility in organizing, drafting, and polishing the project proposal and final report. She also contributed to the implementation including coding, debugging and refactoring. Together, her and Sanjana contributed equally to developing research ideas, defining reward functions, and determining which metrics to collect. Both contributed equally to planning and producing the presentation video, as well as evaluating other groups’ submissions.</p> <h3 id="52-sanjana-nandi">5.2 Sanjana Nandi</h3> <p>Sanjana took primary responsibility in modifying the official implementation of MAPPO <a class="citation" href="#lowe2017multi">(Lowe et al., 2017)</a>, training the model, and gathering data. She also contributed to drafting and polishing the project proposal and final report. Together, her and Juji contributed equally to developing research ideas, defining reward functions, and determining which metrics to collect. Both contributed equally to planning and producing the presentation video, as well as evaluating other groups’ submissions.</p> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Sanjana Nandi. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>